{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<center>\n",
    "<h1>COSC 6336 - Natural Language Processing</h1>\n",
    "<h1>Midterm Exam</h1>\n",
    "</center>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## General instructions\n",
    "\n",
    "The exam covers the implementation of **Naive Bayes** from scratch to classify movie reviews. Each section will provide points for small functions along the implementation. The grader will use those functions to evaluate your solutions.\n",
    "\n",
    "**You are not allowed to use external libraries** such as NLTK. However, **you can use built-in functions and data structures** from Python such as `Counters`, `defaultdict`, `itertools`, and others.\n",
    "\n",
    "The exam was designed to be completed in less than **two hours**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grader import Grader\n",
    "grader = Grader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie Reviews Dataset\n",
    "\n",
    "You are going to use reviews from movies for this exam. You have to predict if a review is positive or negative. Positive is represented by the string `'1'` and negative by the string `'0'`.\n",
    "\n",
    "One data sample is given by a single string containing the review (i.e., the input), and a string with the number of stars for that review (i.e., the output). For instance, the following loop shows the samples from 100 to 104:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:\t 0\n",
      "Review:\t is little more\n",
      "\n",
      "Label:\t 0\n",
      "Review:\t predisposed to the movie 's rude and crude humor\n",
      "\n",
      "Label:\t 1\n",
      "Review:\t see something that did n't talk down to them\n",
      "\n",
      "Label:\t 0\n",
      "Review:\t that 's not the least bit romantic and only mildly funny\n",
      "\n",
      "Label:\t 0\n",
      "Review:\t than Georgia asphalt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import utilities as utils\n",
    "\n",
    "train_orig_sents, train_labels = utils.read_train_data()\n",
    "test_orig_sents, test_labels = utils.read_test_data()\n",
    "\n",
    "for i in range(100, 105):\n",
    "    print('Label:\\t', train_labels[i])\n",
    "    print('Review:\\t', train_orig_sents[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**: Note that the reviews are **already tokenized** inside the string, you don't have to take care of those details when you split the string (e.g., separating commas from words, or tokenizing possesive tokens such as 's, etc). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "Recall that Naive Bayes is a statistical model that uses the Bayesian inference to predict a class. Also remember that, when it comes to classification, we don't need the posterior (denominator) of Bayes' rule:\n",
    "<h3>\n",
    "<center>\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\hat c & = \\underset{c∈C}{\\operatorname{argmax}} P(c~|~d) \\\\\n",
    "       & = \\underset{c∈C}{\\operatorname{argmax}} P(c)~P(d~|~c) \\\\\n",
    "       & = \\underset{c∈C}{\\operatorname{argmax}} P(c)~P(t_1, t_2, ..., t_N~|~c) \\\\\n",
    "       & = \\underset{c∈C}{\\operatorname{argmax}} P(c)~P(t_1~|~c)~P(t_2~|~c)\\cdots P(t_N~|~c) \\\\\n",
    "       & = \\underset{c∈C}{\\operatorname{argmax}} P(c)~\\prod_{i}^{N}{P(t_i~|~c)}\\\\\n",
    "\\hat c & = \\underset{c∈C}{\\operatorname{argmax}} log~P(c)+\\sum_{i}^{N}{log~P(t_i~|~c)}\\\\\n",
    "\\end{align} \n",
    "$$\n",
    "</center>\n",
    "</h3>\n",
    "\n",
    "You will use **log space** to calculate the most likely class (**last equation**). You will break down the NB implementation as follows:\n",
    "\n",
    "* **Step 1**: Calculate the prior probability of the classes: $P(c) = \\frac{N_c}{N_{reviews}}$\n",
    "\n",
    "* **Step 2**: Count the occurrences of the token $t_i$ with class $c$: $~count(t_i, c)$\n",
    "\n",
    "* **Step 3**: Count all the tokens $t$ across the dataset that occur with class $c$: $~\\sum_{t' ∈ D}{count(t', c)}$\n",
    "\n",
    "* **Step 4**: Calculate the likelihood probability of token $t_i$ given the class $c$ using step 2 and step 3: $P(t_i~|~c) = \\frac{count(t_i, c)}{\\sum_{t' ∈ D}{count(t', c)}}$  \n",
    "\n",
    "* **Step 5**: Calculate the log probability of a candidate class $c$ given a review (sequence of tokens): $P(c~|~t_1, t_2, ..., t_N) = log~P(c)+\\sum_{i}^{N}{log~P(t_i~|~c)}$ \n",
    "* **Step 6**: For a single review, do step 5 for all the classes and retrieve the one with the highest probability: $argmax~P(c~|~t_1, t_2, ..., t_N)$\n",
    "\n",
    "* **Step 7**: Do step 6 for all the reviews\n",
    "\n",
    "Keep those steps in mind along the implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [3 pts] 1. Convert each string review into a list of words\n",
    "\n",
    "Do the following for each review (one review at a time):\n",
    "* **Split** it into tokens and **lowercase** the tokens\n",
    "* Ignore the tokens that appear in the set **`stopwords`**\n",
    "* Ignore the tokens that only contain punctuations using the **`RE_PUNCT`** regex\n",
    " \n",
    "Example:\n",
    "* `INPUT : 'This is ANOTHER Age-restricted MOVIE !'` \n",
    "* `OUTPUT: ['another', 'age-restricted', 'movie']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from utilities import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "RE_PUNCT = re.compile(r'^[{}]+$'.format(punctuation)) #### complete the regex \n",
    "def tokenize(sentence):\n",
    "    \"\"\"Return a list of tokens\"\"\"\n",
    "    tokens = sentence.strip().lower().split()\n",
    "    tokenized = []\n",
    "    #### your code goes here\n",
    "    for token in tokens:\n",
    "        if not RE_PUNCT.search(token) and token not in stopwords:\n",
    "            tokenized.append(token)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['another', 'age-restricted', 'movie']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from utilities import stopwords\n",
    "from string import punctuation\n",
    "input = 'This is ANOTHER Age-restricted MOVIE !'\n",
    "tokens = input.strip().lower().split()\n",
    "RE_PUNC = re.compile(r'^[{}]+$'.format(punctuation))\n",
    "a = []\n",
    "for token in tokens:\n",
    "    if not RE_PUNC.search(token) and token not in stopwords:\n",
    "        a.append(token)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not change this!\n",
    "train_sents = [tokenize(sent) for sent in train_orig_sents]\n",
    "test_sents  = [tokenize(sent) for sent in test_orig_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.set_solution('ex_1', tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2 pts] 2. Get the vocabulary of the training dataset\n",
    "\n",
    "You will use the vocabulary size later to smooth the probabilities with Laplace Smoothing. \n",
    "\n",
    "Example:\n",
    "* `INPUT : [['bad', 'review'], ['good', 'movie', 'review']]`\n",
    "* `OUTPUT: {'bad', 'good', 'movie', 'review'}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(sentences):\n",
    "    \"\"\"Return a set with the vocabulary\"\"\"\n",
    "    words = []\n",
    "    #### your code goes here\n",
    "    for items in sentences:\n",
    "        for item in items:\n",
    "            words.append(item)\n",
    "    return set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not change this!\n",
    "vocab = get_vocab(train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grader.set_solution('ex_2', vocab)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [3 pts] 3. Get the prior probabilities of the classes (step 1)\n",
    "\n",
    "You can calculate $P(c) = \\frac{N_c}{N_{reviews}}$ by counting the occurrences of the classes and then normalizing those counts by the total number of occurrences. \n",
    "\n",
    "Example:\n",
    "* `INPUT : ['0', '1', '1', '1', '0', '0']`\n",
    "* `OUTPUT: {'0': 0.5, '1': 0.5}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_prior_probabilities(labels):\n",
    "    \"\"\"Return a dictionary with classes as keys and probabilities as values\"\"\"\n",
    "    counts = dict()\n",
    "    a = len(labels)\n",
    "    for item in labels:\n",
    "        if item not in counts:\n",
    "            counts[item] = 1\n",
    "        else:\n",
    "            counts[item] +=1\n",
    "    for item in counts:\n",
    "        counts[item] =counts[item]/a\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not change this!\n",
    "prior_probs = get_prior_probabilities(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.set_solution('ex_3', get_prior_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [3 pts] 4.  Count the occurrences of the tuple `(token, class)` (step 2)\n",
    "\n",
    "\n",
    "To handle step 2, $~count(t_i, c)$, return a dictionary whose keys are tuples of the shape (token, class) and the values are the frequencies of the tuples. \n",
    "\n",
    "Example:\n",
    "* `INPUT 1 (sents): [['bad', 'review'], ['good', 'review'], ['good', 'movie']]`\n",
    "* `INPUT 2 (labels): ['0', '1', '1']`\n",
    "* `OUTPUT: {('bad', '0'): 1, ('good', '1'): 2, ('movie', '1'): 1, ('review', '0'): 1, ('review', '1'): 1}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_label_counts(sents, labels):\n",
    "    \"\"\"Return a dictionary with a tuple (token ,class) as keys and the frequencies as values\"\"\"\n",
    "    counts = dict()\n",
    "    for i in range(len(sents)):\n",
    "        for item in sents[i]:\n",
    "            tuple1 = (item,labels[i])\n",
    "            if tuple1 not in counts:\n",
    "                counts[tuple1] = 1\n",
    "            else:\n",
    "                counts[tuple1] += 1\n",
    "    return counts\n",
    "\n",
    "word_label_counts = get_word_label_counts(train_sents, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_label_counts = get_word_label_counts(train_sents, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.set_solution('ex_4', word_label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4 pts] 5. Count all the tokens t across the training dataset that occur with class c (step 3)\n",
    "\n",
    "To handle step 3, $~\\sum_{t' ∈ D}{count(t', c)}$, return a dictionary whose keys are the classes and the values are the number of words (across all the training set) that appear under that class. \n",
    "\n",
    "Example:\n",
    "* `INPUT 1 (sents): [['bad', 'review'], ['good', 'review'], ['good', 'movie']]`\n",
    "* `INPUT 2 (labels): ['0', '1', '1']`\n",
    "* `OUTPUT: {'0': 2, '1': 4}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_number_of_word_label(sents, labels):\n",
    "    \"\"\"Return a dictionary whose keys are the classes and the values are the number of words seen with that class\"\"\"\n",
    "    counts = dict()\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] not in counts:\n",
    "            counts[labels[i]] = len(sents[i])\n",
    "        else:\n",
    "            counts[labels[i]] += len(sents[i])\n",
    "    return counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_by_label_counts = get_number_of_word_label(train_sents, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.set_solution('ex_5', words_by_label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [3 pts] 6. Calculate the likelihood estimation for a word given a class (step 4)\n",
    "\n",
    "You can use the dictionaries from the two previous questions to calculate $P(t_i~|~c) = \\frac{count(t_i, c)}{\\sum_{t' ∈ D}{count(t', c)}}$  \n",
    "\n",
    "Example:\n",
    "* `INPUT 1 (token): 'movie'`\n",
    "* `INPUT 2 (label): '0'`\n",
    "* `OUTPUT: 0.009456842481429221`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_likelihood(word, label):\n",
    "    \"\"\"Given a token and a label, retrieve the likelihood probability (float)\"\"\"\n",
    "    #### your code goes here\n",
    "    a = 0\n",
    "    b = 0\n",
    "    for item in word_label_counts:\n",
    "        if word == item[0]:\n",
    "            a = word_label_counts[item]\n",
    "            b = words_by_label_counts[item[1]]\n",
    "    return a/b if b else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.set_solution('ex_6', get_likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2 pts] 7. Add Laplace Smoothing to the previous function\n",
    "\n",
    "Copy your approach and paste it into this function. Then add Laplace smoothing\n",
    "\n",
    "Example:\n",
    "* `INPUT 1 (token): 'movie'`\n",
    "* `INPUT 2 (label): '0'`\n",
    "* `OUTPUT: 0.009069266523069946`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_likelihood_with_smoothing(word, label):\n",
    "    \"\"\"Given a token and a label, retrieve the smoothed likelihood probability (float)\"\"\"\n",
    "    #### your code goes here\n",
    "    c = 0\n",
    "    for item in word_label_counts:\n",
    "        if word == item[0]:\n",
    "            if word_label_counts[item] == 0:\n",
    "                c = 1/(words_by_label_counts[item[1]]+len(vocab))\n",
    "            else:\n",
    "                c =(word_label_counts[item]+1)/(words_by_label_counts[item[1]]+len(vocab))\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.set_solution('ex_7', get_likelihood_with_smoothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [3 pts] 8. Calculate the probability of a class given a review (list of words) (step 5)\n",
    "\n",
    "Given a review, calculate the probability of a candidate class. Remember that this is achieved by $P(c~|~t_1, t_2, ..., t_N) = log~P(c)+\\sum_{i}^{N}{log~P(t_i~|~c)}$ \n",
    "\n",
    "**NOTE:** Use **log space**, and the **`get_likelihood_with_smoothing`** function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "def get_class_prob_given_a_review(review, label):\n",
    "    \"\"\"Return the probability of a class given a list of tokens\"\"\"\n",
    "    ### your code goes here \n",
    "    sum = 0\n",
    "    pc = 0\n",
    "    for word in review:\n",
    "        sum += log(get_likelihood_with_smoothing(word, label))\n",
    "    pc = log(prior_probs(label))\n",
    "    return sum+pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.set_solution('ex_8', get_class_prob_given_a_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [3 pts] 9. Get the class with the highest probabilities for a single review (step 6)\n",
    "\n",
    "Now you have to iterate over all the classes and get the probability of the class given a review. You will then need to keep the highest probability to find the most likely class: $argmax~P(c~|~t_1, t_2, ..., t_N)$\n",
    "\n",
    "**Hint**: use the previous function **`get_class_prob_given_a_review`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_review(review):\n",
    "    \"\"\"Return the most likely label for the list of tokens\"\"\"\n",
    "    pred_label = ''\n",
    "    pred_prob  = float('-inf')\n",
    "    ### your code goes here for k in aDict.iterkeys():\n",
    "    for (label, prob) in prior_probs.items():\n",
    "        if pred_prob < get_class_prob_given_a_review(review, label):\n",
    "            pred_prob = get_class_prob_given_a_review(review, label)\n",
    "            pred_label = label\n",
    "    return pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.set_solution('ex_9', classify_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1 pts] 10. Classify reviews for all the sentences (step 7)\n",
    "\n",
    "Now you have to get the most likely class for all the reviews. Use the previous function (ex 9, step 6) and apply that for all the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_reviews(reviews):\n",
    "    \"\"\"Return a list with predictions for all the reviews\"\"\"\n",
    "    predictions = []\n",
    "    ### your code goes here\n",
    "    for review in reviews:\n",
    "        predictions.append(classify_review(review))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dict' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-8de77f88a160>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### your code goes here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassify_reviews\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-8692e09d3a93>\u001b[0m in \u001b[0;36mclassify_reviews\u001b[0;34m(reviews)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m### your code goes here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreviews\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassify_review\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-a6256cd05e2b>\u001b[0m in \u001b[0;36mclassify_review\u001b[0;34m(review)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m### your code goes here for k in aDict.iterkeys():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprior_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mpred_prob\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mget_class_prob_given_a_review\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0mpred_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_class_prob_given_a_review\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mpred_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-12d755588fd0>\u001b[0m in \u001b[0;36mget_class_prob_given_a_review\u001b[0;34m(review, label)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreview\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0msum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_likelihood_with_smoothing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mpc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior_probs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'dict' object is not callable"
     ]
    }
   ],
   "source": [
    "### your code goes here \n",
    "predictions = classify_reviews(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.set_solution('ex_10', classify_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2 pts] 11. Check accuracy of your classifier\n",
    "\n",
    "Complete the accuracy function and evaluate your model. Recall that the accuracy is defined as follows:\n",
    "\n",
    "$$\n",
    "Accuracy = \\frac{TP+TN}{TP+TN+FP+FN}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(actual_labels, predicted_labels):\n",
    "    right = 0\n",
    "    total = 0\n",
    "    #### your code goes here\n",
    "    for label2 in predicted_labels:\n",
    "        if label2 in actual_labels:\n",
    "            right +=1\n",
    "            total +=1\n",
    "        else: \n",
    "            total +=1\n",
    "    return right / total if total else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.set_solution('ex_11', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grade:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ex_1:\t3 / 3\n",
      "ex_2:\t2 / 2\n",
      "ex_7:\t0 / 2\n",
      "ex_4:\t3 / 3\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'ex_10'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-5bdb85d675a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrade\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Spring 2018/midterm/grader.py\u001b[0m in \u001b[0;36mgrade\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mex_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                 \u001b[0maward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mex_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}:\\t{} / {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \u001b[0mright\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0maward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Spring 2018/midterm/grader.py\u001b[0m in \u001b[0;36m_ex110\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'this is a good review'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'this is a bad review'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mexpected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0msolution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_solutions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ex_10'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msolution\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mexpected\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_points\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ex_10'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ex_10'"
     ]
    }
   ],
   "source": [
    "grader.grade()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
