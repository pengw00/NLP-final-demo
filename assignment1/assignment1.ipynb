{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# In-class Assignment 1\n",
    "\n",
    "This assignment will first cover regular expressions, a.k.a RegEx. After that, we will make use of them to tokenize and normalize two different types of text. You can always check chapter 2 to make sure what operators you need for a specific task.\n",
    "\n",
    "Let's start with some RegEx exercises!\n",
    "\n",
    "## 1. Regular Expressions\n",
    "\n",
    "We will use data from Twitter for this section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lets have a date me and you for ever and ever üòçüòò #NeverAlone\r\n",
      "ni el header he hecho....\r\n",
      "va tener un hijo...confrsion es mioesta urbanisacion es enorme...\r\n",
      "deseo acabar con todo...\r\n",
      "@NEWdanicsierra em que?\r\n"
     ]
    }
   ],
   "source": [
    "twitter_file = 'twitter.txt'\n",
    "!head -5 {twitter_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 1.1. Using grep, find all the tweets (lines) that start with capital letters and either have a twitter username (@username) or a hashtag at the end.\n",
    "\n",
    "**Hint**: you should get 26 lines. Use the pipe operator to count the lines: ``!grep ... | wc -l``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lets have a date me and you for ever and ever üòçüòò #NeverAlone',\n",
       " 'Me encantaria que me vieras, de la manera que yo te veo #OneDay',\n",
       " \"Going asleep with papi's music on skype, even thoe our call hang up #Priceless #DistanceDontMatter\",\n",
       " 'Que rico me duele mi cuerpo #goodexercise',\n",
       " 'La conversaci√≥n de precol mas vale que continue! @mememecaigo @PattyB_14  @salim_rosa',\n",
       " 'Ya sabes mi bro arjona me gusta üòäüòè @Abnerdm',\n",
       " 'I feel so sad üòîüòû #GodPleaseGuideMe',\n",
       " 'Movie is about to begin #SonOfGod',\n",
       " 'I hate the fact people get into my room and stuff start missing üòîüòÅüò© #WTH',\n",
       " 'FULL RIDE :o #PARY',\n",
       " 'I got 96 in my test #purocerebro',\n",
       " 'Nike is making some pretty kiks #ilovethem',\n",
       " 'DONT LIE TO ME ‚ùåüö´üòÅ #backofme',\n",
       " 'That sucks! But I like #MatthewMcConaughey',\n",
       " 'I wanna go to sleep but I gotta see #Leo win that first Oscar #Oscars',\n",
       " 'I take naps serious ..digo 15 mins. me le anto 3 horas despues-.- #always',\n",
       " \"I'm always showing off @ReguloCaro 's daughter as if she's my own haha. #cutestkidevva\",\n",
       " \"Okay the match is over, horrible refereeing that definitely benefited Real Madrid. That's soccer for you. I'll take the tie (2-2) #lideres\",\n",
       " 'I must wake up early tomorrow! #mustgetworkdone',\n",
       " 'Hoy en ingles .tidos somos carlos perez! jajaja @nataliesolange  @mememecaigo  @LoubrielClaudia',\n",
       " \"Happy B-Day to my boi @DCB4L y'all be sure to wish the homie a Happy B-Day! #TrillHomie\",\n",
       " \"Already my boi! Salute from da Dirty D! Dallas Texas fucks wit y'all FLA Mexicanos! Y'all check out my boi @CityboiChiko561 jam! Jammin@RepDeportiva 4 veces #RepGana\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex11 = !grep -E \"^[A-Z].*(#[a-zA-Z0-9_]+|@[a-zA-Z0-9_]+)$\" {twitter_file}\n",
    "ex11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 1.2. Do the same exercise but using Python this time. \n",
    "\n",
    "**Hint**: the regular expression should be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lets have a date me and you for ever and ever üòçüòò #NeverAlone',\n",
       " 'Me encantaria que me vieras, de la manera que yo te veo #OneDay',\n",
       " \"Going asleep with papi's music on skype, even thoe our call hang up #Priceless #DistanceDontMatter\",\n",
       " 'Que rico me duele mi cuerpo #goodexercise',\n",
       " 'La conversaci√≥n de precol mas vale que continue! @mememecaigo @PattyB_14  @salim_rosa',\n",
       " 'Ya sabes mi bro arjona me gusta üòäüòè @Abnerdm',\n",
       " 'I feel so sad üòîüòû #GodPleaseGuideMe',\n",
       " 'Movie is about to begin #SonOfGod',\n",
       " 'I hate the fact people get into my room and stuff start missing üòîüòÅüò© #WTH',\n",
       " 'FULL RIDE :o #PARY',\n",
       " 'I got 96 in my test #purocerebro',\n",
       " 'Nike is making some pretty kiks #ilovethem',\n",
       " 'DONT LIE TO ME ‚ùåüö´üòÅ #backofme',\n",
       " 'That sucks! But I like #MatthewMcConaughey',\n",
       " 'I wanna go to sleep but I gotta see #Leo win that first Oscar #Oscars',\n",
       " 'I take naps serious ..digo 15 mins. me le anto 3 horas despues-.- #always',\n",
       " \"I'm always showing off @ReguloCaro 's daughter as if she's my own haha. #cutestkidevva\",\n",
       " \"Okay the match is over, horrible refereeing that definitely benefited Real Madrid. That's soccer for you. I'll take the tie (2-2) #lideres\",\n",
       " 'I must wake up early tomorrow! #mustgetworkdone',\n",
       " 'Hoy en ingles .tidos somos carlos perez! jajaja @nataliesolange  @mememecaigo  @LoubrielClaudia',\n",
       " \"Happy B-Day to my boi @DCB4L y'all be sure to wish the homie a Happy B-Day! #TrillHomie\",\n",
       " \"Already my boi! Salute from da Dirty D! Dallas Texas fucks wit y'all FLA Mexicanos! Y'all check out my boi @CityboiChiko561 jam! Jammin@RepDeportiva 4 veces #RepGana\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def read_file(file):\n",
    "    return [l.rstrip() for l in open(file)]\n",
    "\n",
    "def ex12(file):\n",
    "    \"\"\"Return the lines that match the regex pattern you did in the previous exercise\"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    lines = read_file(file)\n",
    "    list = []\n",
    "    for i in range(len(lines)):\n",
    "        r = re.compile('^[A-Z].*(#[a-zA-Z0-9_]+|@[a-zA-Z0-9_]+)$')\n",
    "        j = r.findall(lines[i])\n",
    "        if len(j) != 0:\n",
    "             list.append(lines[i])\n",
    "    return list\n",
    "    \"\"\"\n",
    "    pattern = r'^[A-Z].*(@[a-zA-Z0-9_]+|#[A-Za-z0-9_]+)$'\n",
    "    return [l for l in read_file(file) if re.findall(pattern, l)]\n",
    "    print(len(ex12_sol(twitter_file)))\n",
    "ex12(twitter_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 1.3. Find all the tweets that contains emojis\n",
    "\n",
    "**Hint**: Use the following unicode block: `x1F000 - x2F000`.\n",
    "\n",
    "If we were to search using grep, we would need to keep in mind that the file is written in UTF-8, and as such we need to use the hexadecimal values of the bytes in the regex pattern (you can use this link to see the conversion: https://r12a.github.io/apps/conversion/). For our unicode block we would have the following:\n",
    "* `x1F000 -> F0 9F 80 80`\n",
    "* `x2F000 -> F0 AF 80 80`\n",
    "\n",
    "The command to grep a specific unicode in bytes would be like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚Äú@itseffinjas: I'm eating like I'm pregnant üò≥‚Äù Maybe u is\r\n"
     ]
    }
   ],
   "source": [
    "!grep \"\"$'\\xF0\\x9F\\x98\\xB3'\"\" {twitter_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's do the search for all the emojis but this time using Python (you won't need the byte representation this time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "def ex13(file):\n",
    "    \"\"\"Return the lines that contain emojis\"\"\"\n",
    "    lines = read_file(file)\n",
    "    inblock = lambda c:int('01F000', 16) <= ord(c) <= int('02F000',16)\n",
    "    return [ l for l in lines if any(map(inblock,1))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. Tokenization\n",
    "\n",
    "Tokenization can get complicated with a lot of edge cases. This time, we will just implement a toy tokenization function. Your function need to consider the following cases:\n",
    "* Split by spaces, tabs, and enters:\n",
    "    * `'  this is a tweet   ' -> 'this', 'is', 'a', 'tweet'`\n",
    "* Split by emojis (keeping them):\n",
    "    * `haha üòÇüòÇ! -> 'haha', 'üòÇüòÇ', '!'`\n",
    "* Keep hashtags intact (i.e., do not split inside a hashtag):\n",
    "    * `#don'tSplit `\n",
    "* Separate punctuation marks from the text as individual tokens:\n",
    "    * `'hey!' -> 'hey', '!'`\n",
    "* Contractions need to be separated based on the apostrophe (without removing it):\n",
    "    * `\"that's fine\" -> 'that', \"'s\", 'fine'`\n",
    "    * `\"don't do that\" -> 'do', \"n't\", 'do', 'that'`\n",
    "\n",
    "For contractions, consider the following ending cases:\n",
    "* `'s`\n",
    "* `n't`\n",
    "* `'ll`\n",
    "* `'m`\n",
    "* `'ve`\n",
    "* `'d`\n",
    "* `'re`\n",
    "* `'all`\n",
    "\n",
    "**Hint**: Split using a cascade approach. That is, do not try to cover all the cases at once ‚Äîdivide and conquer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 're' has no attribute 'research'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6bf274e1beb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mex2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mex2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtwitter_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-6bf274e1beb7>\u001b[0m in \u001b[0;36mex2\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mex2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mex2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtwitter_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-6bf274e1beb7>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mex2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mex2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtwitter_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-6bf274e1beb7>\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"^[@#]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[!?.,;(...):]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m               \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"(!|?|.|;|...)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 're' has no attribute 'research'"
     ]
    }
   ],
   "source": [
    "def tokenize(sentence):\n",
    "    \"\"\"Return the tokens of the sentence\"\"\"\n",
    "    tokens = sentence.strip().split()\n",
    "\n",
    "    i = []\n",
    "    for token in tokens:\n",
    "        if not re.research(\"^[@#]\", token):\n",
    "            if re.search(\"[!?.,;(...):]\", token):\n",
    "              k = re.split(\"(!|?|.|;|...)\")\n",
    "            for eachone in k:\n",
    "                h = re.split(\"\\w+(?=n't)\")\n",
    "                j.extend(smallest_tokens)\n",
    "        else:\n",
    "            j.append(token)\n",
    "    return tokens\n",
    "def ex2(file):\n",
    "    return [tokenize(l) for l in read_file(file)]\n",
    "ex2(twitter_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['grep: Invalid collation character']"
      ]
     },
     "execution_count": 1,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = !grep \"[\"$'\\xF0\\x9F\\x80\\x80'\"-\"$'\\xF0\\xAF\\x80\\x80'\"]\" twitter.txt \n",
    "\n",
    "print(len(res))\n",
    "res[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. Normalization\n",
    "\n",
    "Based on the previous tokenization, we will perform a normalization step. To keep it simple, we will only handle the case in which a sequence of letters has a repeated pattern. Look at the following examples:\n",
    "* `hahahahaha -> haha`\n",
    "* `cooooool -> cool`\n",
    "* `!?!?!?!!!! -> !?!?!!`\n",
    "* `!!!!!????? -> !!??`\n",
    "\n",
    "**Hint:** Note that the reapeated patterns are reduce to 2 ocurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def ex3(tokenized_tweets):\n",
    "    \"\"\"Return a list of list where the inner items are the normalized tokens of each tweet\"\"\"\n",
    "\n",
    "    #\n",
    "\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ex11' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4604432c55af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mgrader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-4604432c55af>\u001b[0m in \u001b[0;36mgrader\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgrader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mexs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mgrade_ex11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrade_ex12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrade_ex13\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrade_ex2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrade_ex3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mgrader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-4604432c55af>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(i)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgrader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mexs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mgrade_ex11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrade_ex12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrade_ex13\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrade_ex2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrade_ex3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mgrader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-4604432c55af>\u001b[0m in \u001b[0;36mgrade_ex11\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgrade_ex11\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0msolution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m63\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m98\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m41\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m85\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m39\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m43\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m34\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m33\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m133\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m72\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m18\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m58\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m69\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m73\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m26\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m86\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m138\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m31\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m47\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m95\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m87\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m165\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mex11\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msolution\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ex11' is not defined"
     ]
    }
   ],
   "source": [
    "def grade_ex11():\n",
    "    solution = [60, 63, 98, 41, 85, 39, 43, 34, 33, 133, 72, 18, 32, 58, 42, 28, 42, 69, 73, 26, 86, 138, 31, 47, 95, 87, 165]\n",
    "    result = [len(t) for t in ex11]\n",
    "    return solution == result\n",
    "\n",
    "def grade_ex12():\n",
    "    solution = [60, 63, 98, 41, 85, 39, 43, 34, 33, 133, 72, 18, 32, 58, 42, 28, 42, 69, 73, 26, 86, 138, 31, 47, 95, 87, 165]\n",
    "    result = [len(t) for t in ex12(twitter_file)]\n",
    "    return solution == result\n",
    "\n",
    "def grade_ex13():\n",
    "    solution = [60, 65, 37, 20, 24, 18, 63, 117, 132, 39, 73, 43, 14, 55, 33, 18, 47, 43, 92, 54, 34, 31, 97, 55, 17, 73, 126, 45, 133, 72, 39, 41, 24, 58, 23, 25, 20, 37, 31, 33, 28, 20, 24, 82, 83, 114, 134, 72, 78, 139, 101, 94, 139, 46, 31, 16, 19, 18, 22, 35, 49, 5, 55, 34, 60, 137, 40, 54, 118, 19, 57, 43]\n",
    "    result = [len(t) for t in ex13(twitter_file)]\n",
    "    if len(solution) <= len(result) <= len(solution) + 10: # margin of error\n",
    "        return all([tweet in result for tweet in solution])\n",
    "    return False\n",
    "\n",
    "def grade_ex2():\n",
    "    solution = [14, 6, 12, 5, 4, 11, 16, 14, 16, 10, 10, 8, 2, 4, 14, 11, 15, 6, 5, 8, 13, 22, 8, 5, 10, 8, 15, 16, 6, 9, 7, 7, 10, 15, 7, 9, 24, 4, 13, 3, 2, 21, 11, 17, 6, 15, 4, 19, 7, 11, 11, 6, 4, 4, 3, 2, 8, 4, 5, 17, 5, 16, 7, 6, 6, 6, 15, 11, 12, 7, 12, 7, 10, 2, 4, 10, 5, 2, 15, 5, 3, 12, 5, 18, 15, 4, 23, 8, 4, 4, 9, 11, 1, 3, 9, 5, 7, 11, 4, 12, 5, 4, 5, 5, 7, 5, 3, 20, 11, 10, 6, 12, 14, 6, 10, 7, 8, 4, 8, 9, 10, 19, 14, 7, 8, 5, 16, 15, 2, 10, 6, 16, 8, 5, 17, 23, 10, 26, 16, 31, 9, 3, 18, 23, 9, 9, 21, 12, 2, 8, 5, 2, 4, 8, 9, 6, 7, 7, 21, 3, 11, 9, 4, 10, 13, 8, 24, 5, 6, 7, 12, 8, 9, 8, 8, 8, 5, 11, 11, 17, 2, 8, 20, 8, 9, 9, 2, 5, 8, 11, 3, 13, 6, 13, 2, 16, 15, 7, 4, 8, 14, 5, 5, 4, 2, 9, 6, 25, 29, 11, 5, 18, 11, 5, 2, 9, 3, 10, 11, 19, 19, 17, 7, 17, 4, 30, 14, 26, 6, 4, 11, 12, 2, 4, 12, 12, 4, 12, 3, 10, 3, 19, 8, 8, 19, 10, 14, 25, 10, 8, 16, 12, 5, 10, 6, 5, 12, 8, 4, 3, 10, 7, 12, 22, 9, 12, 7, 5, 10, 26, 11, 6, 6, 5, 11, 4, 16, 12, 32, 11, 24, 29, 17, 12, 11, 3, 1, 2, 4, 7, 3, 7, 14, 10, 4, 11, 10, 6, 13, 9, 12, 15, 4, 3, 5, 6, 3, 12, 6, 3, 6, 8, 10, 11, 5, 4, 8, 6, 10, 4, 8, 2, 7, 3, 7, 5, 13, 7, 7, 4, 4, 4, 16, 6, 3, 4, 10, 5, 8, 7, 1, 2, 9, 3, 4, 5, 6, 8, 10, 7, 7, 5, 4, 2, 6, 21, 1, 4, 16, 4, 9, 16, 2, 3, 9, 4, 9, 10, 2, 20, 7, 6, 8, 3, 14, 4, 12, 4, 5, 12, 7, 7, 4, 10, 31, 10, 2, 6, 18, 28, 4, 14, 12, 6, 15, 18, 6, 7, 8, 9, 8, 4, 9, 32, 7, 9, 8, 5, 10, 7, 12, 12, 35, 2, 12, 2, 7, 8, 11, 5, 13, 23, 7, 13, 18, 8, 10, 13, 24, 32, 7, 15, 9, 11, 2, 4, 14, 14, 11, 3, 11, 2, 12, 6, 15, 8, 10, 10, 7, 2, 4, 4, 19, 3, 23, 17, 14, 8, 21, 10, 5, 16, 10, 12, 14, 15, 7, 13, 24, 5, 12, 36, 15, 6, 15, 19, 11, 13, 7, 11, 23, 15, 9, 10, 19, 10, 12, 29, 14, 19, 16, 22, 12, 7, 11, 26, 13, 7, 7, 9, 13, 14, 17, 8, 10, 10, 13, 21, 8, 10, 16, 17, 13, 13, 6, 10, 28, 5, 11, 10, 6, 19, 17, 22, 22, 9, 14, 9, 19, 11, 10, 15, 24, 31, 16, 6, 9, 16, 14, 10, 14, 23, 9, 17, 8, 8, 26, 13, 25, 19, 32, 8, 10, 18, 9, 13, 5, 17, 4, 18, 16, 8, 11, 9, 10, 22, 15, 13, 24, 17, 16, 18, 7, 9, 29, 16, 9, 24, 9, 11, 13, 10, 7, 4, 20, 11, 8, 27, 5, 13, 17, 12, 10, 8, 12, 4, 19, 19, 12, 10, 8, 13, 20, 12, 9, 12, 14, 10, 25, 16, 14, 7, 18, 20, 14, 22, 23, 23, 10, 12, 5, 11, 19, 14, 10, 10, 7, 23, 27, 12, 15, 20, 9, 15, 13, 16, 7, 7, 14, 18, 14, 29, 12, 13, 12, 11, 19, 8, 12, 16, 9, 7, 9, 5, 20, 10, 26, 10, 24, 7, 10, 25, 14, 10, 24, 14, 16, 29, 5, 35, 17, 12, 7, 14, 28, 7, 33, 7, 14, 18, 6, 16, 4, 9, 12, 18, 4, 11, 15, 14, 13, 19, 12, 13, 10, 14, 12, 6, 23, 22, 8, 17, 13, 14, 15, 27, 14, 10, 4, 6, 4, 5, 7, 6, 7, 10, 4, 11, 5, 8, 9, 8, 9, 6, 3, 7, 5, 8, 10, 11, 6, 2, 11, 5, 6, 7, 20, 7, 12, 19, 5, 15, 12, 6, 8, 25, 2, 13, 14, 25, 7, 12, 19, 12, 2, 10, 7, 4, 11, 9, 9, 13, 6, 15, 13, 6, 12, 10, 5, 4, 7, 8, 18, 9, 6, 7, 11, 4, 7, 9, 20, 7, 24, 9, 9, 10, 9, 30, 15, 4, 5, 5, 15, 11, 10, 24, 4, 11, 23, 16, 32, 9, 10, 13, 20, 7, 22, 21, 12, 32, 26, 24, 10, 19, 12, 17, 4, 32, 8, 12, 4, 15, 10, 23]\n",
    "    result = [len(t) for t in ex2(twitter_file)]\n",
    "    total = len(solution)\n",
    "    correct = len([i for i in range(len(solution)) if (solution[i] -2) <= result[i] <= (solution[i] + 2)])\n",
    "    return (correct / total) > 0.95\n",
    "\n",
    "def grade_ex3():\n",
    "    test_cases = [['hahahahaha', 'cooooool', '!?!?!?!!!!', '!!!!!?????']]\n",
    "    res_cases = [['haha', 'cool', '!?!?!!', '!!??']]\n",
    "    return res_cases == ex3(test_cases)\n",
    "\n",
    "def grader():\n",
    "    exs = [ grade_ex11, grade_ex12, grade_ex13, grade_ex2, grade_ex3]\n",
    "    return 100 * len(list(filter(lambda i:i(), exs))) / len(exs)\n",
    "\n",
    "grader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Anaconda)",
   "language": "python",
   "name": "anaconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}