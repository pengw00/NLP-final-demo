{"kernelspec":{"display_name":"Python 3 (Ubuntu Linux)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}}
{"cell_type":"markdown","metadata":{},"source":"---\n\n<center>\n<h1>COSC 6336 - Natural Language Processing</h1>\n<h1>Final Exam</h1>\n</center>\n\n---\n\n\n## General instructions\n\nThe exam is divided into two sections. The first part is all about theory questions, and the second part presents a practical problem where you will need to code. There is no grader this time.\n\nThe exam was designed to be completed in **two hours**. "}
{"cell_type":"code","execution_count":1,"metadata":{"editable":false,"trusted":true},"outputs":[],"source":"from helper import TestManager\ntm = TestManager()"}
{"cell_type":"markdown","metadata":{"editable":false},"source":"## Part 1\n\nAnswer the following questions by choosing one of the answers. Once you are done with all the questions, **run the submit cell** (this will store your answers and avoid potential loss of them).\n\n### 1 - (5pts)\nIn a dependency-based parse tree, which of the following is true?\n    \n    a. All nodes are labeled with words\n    b. Only leaf nodes are labeled with words\n    c. Only non-terminal nodes are labeled with words\n    d. No words appear in the tree"}
{"cell_type":"code","execution_count":2,"metadata":{"editable":false,"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b90228e58c2045d88b0295ad47930989","version_major":2,"version_minor":0},"text/plain":"Dropdown(description='Your choice:', options=('-- Select one', 'Option A', 'Option B', 'Option C', 'Option D')…"},"metadata":{},"output_type":"display_data"}],"source":"tm.display(1)"}
{"cell_type":"markdown","metadata":{"editable":false},"source":"### 2 - (5pts)\nIn a parsing tree, which of the following is true?\n\n    a. Each node represents a word in the sentence\n    b. The order of the words is not preserved\n    c. The number of the leaves is equal to the number of the words\n    d. The tree is always binary"}
{"cell_type":"code","execution_count":60,"metadata":{"editable":false,"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"563476864c6b499bb4a1fbc2220a96a8","version_major":2,"version_minor":0},"text/plain":"Dropdown(description='Your choice:', options=('-- Select one', 'Option A', 'Option B', 'Option C', 'Option D')…"},"metadata":{},"output_type":"display_data"}],"source":"tm.display(2)"}
{"cell_type":"markdown","metadata":{"editable":false},"source":"### 3 - (5pts)\nFrom the linguistic perspective, what is one of the problems of the CKY algorithm?\n\n    a. It is not practical because it is too slow regardless of the grammar\n    b. It is not capable to produce all the possible trees with respect to the grammar for a given sequence\n    c. The grammar has to be in CNF and it will only produce binary trees\n    d. There is no problem with the CKY algorithm from the linguistic perspective"}
{"cell_type":"code","execution_count":54,"metadata":{"editable":false,"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5f74e17c0d224f86a330b53d745c9542","version_major":2,"version_minor":0},"text/plain":"Dropdown(description='Your choice:', options=('-- Select one', 'Option A', 'Option B', 'Option C', 'Option D')…"},"metadata":{},"output_type":"display_data"}],"source":"tm.display(3)"}
{"cell_type":"markdown","metadata":{},"source":"### 4 - (5pts)\nIn the context of the IOB scheme, which of the following is true?\n\n    a. IOB stands for Inside Outside and Between and it is used on chunking tasks \n    b. The IOB scheme is a way to group multiple tokens in a chunking task from a given sequence\n    c. The IOB scheme helps to provide semantic and syntactic information in a classification task\n    d. The IOB scheme is only used for NER labels out of all the NLP applications"}
{"cell_type":"code","execution_count":55,"metadata":{"editable":false,"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3a56e07bb4a949729f15567e9a0ef9fd","version_major":2,"version_minor":0},"text/plain":"Dropdown(description='Your choice:', options=('-- Select one', 'Option A', 'Option B', 'Option C', 'Option D')…"},"metadata":{},"output_type":"display_data"}],"source":"tm.display(4)"}
{"cell_type":"markdown","metadata":{"editable":false},"source":"### 5 - (5pts)\n\nIn a classification task where the data is extremely unbalanced, what would be the most appropiate metric to measure the output of your model in such scenario?\n\n    a. Precision\n    b. Accuracy\n    c. F-measure\n    d. Recall"}
{"cell_type":"code","execution_count":56,"metadata":{"editable":false,"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1da75423f4f948299efc236c19b4ab66","version_major":2,"version_minor":0},"text/plain":"Dropdown(description='Your choice:', options=('-- Select one', 'Option A', 'Option B', 'Option C', 'Option D')…"},"metadata":{},"output_type":"display_data"}],"source":"tm.display(5)"}
{"cell_type":"markdown","metadata":{"editable":false},"source":"### 6 - (5pts)\n\nWhat is the standard way to associate probabilities to the production rules of a gramar? \n\n    a. Calculating the frequency of that production rule and then dividing by the total \n       number of rules in the grammar\n    b. Counting all the rules with the same right-hand-side symbols and then dividing it by \n       the total number of rules in the grammar\n    c. Calculating the frequency of that production rule and then diving by the total \n       number of rules that contain the same head (lef-hand-side) symbol of the original rule\n    d. Counting all the rules with the same right-hand-side symbols regardless of the order\n       and then dividing it by the total number of rules"}
{"cell_type":"code","execution_count":57,"metadata":{"editable":false,"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"10f6a5ce3f16429cb1832502a5de317a","version_major":2,"version_minor":0},"text/plain":"Dropdown(description='Your choice:', options=('-- Select one', 'Option A', 'Option B', 'Option C', 'Option D')…"},"metadata":{},"output_type":"display_data"}],"source":"tm.display(6)"}
{"cell_type":"markdown","metadata":{"editable":false},"source":"### 7 - (5pts)\n\nRegarding the Earley Parsing algorithm, which of the following is **not** correct?\n\n    a. It runs in a top-bottom fashion\n    b. Grammar must be in CNF\n    c. Allows arbitrary CFGs\n    d. The resulting parse tree may be consistent with linguistic grammar"}
{"cell_type":"code","execution_count":58,"metadata":{"editable":false,"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4099207efdc445f790cfc14f39fad415","version_major":2,"version_minor":0},"text/plain":"Dropdown(description='Your choice:', options=('-- Select one', 'Option A', 'Option B', 'Option C', 'Option D')…"},"metadata":{},"output_type":"display_data"}],"source":"tm.display(7)"}
{"cell_type":"markdown","metadata":{"editable":false},"source":"### 8 - (5pts)\n\nIn the context of dialogue systems, which of the following is **not** a limitation/constraint of sequence to sequence models?\n\n    a. They need text preprocessing and lemmatization\n    b. They are originally designed for Machine Translation and thus they disregard some \n       conversational aspects\n    c. They may show a tendency for repetitive responses\n    d. They lack contextual information"}
{"cell_type":"code","execution_count":51,"metadata":{"editable":false,"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f03fe5d6cb6c4d56bce520cf99508037","version_major":2,"version_minor":0},"text/plain":"Dropdown(description='Your choice:', options=('-- Select one', 'Option A', 'Option B', 'Option C', 'Option D')…"},"metadata":{},"output_type":"display_data"}],"source":"tm.display(8)"}
{"cell_type":"markdown","metadata":{"editable":false},"source":"Run the following cell to take a look at your choices you have done so far:"}
{"cell_type":"code","execution_count":61,"metadata":{"editable":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"1 -> A\n2 -> D\n3 -> C\n4 -> A\n5 -> C\n6 -> C\n7 -> C\n8 -> D\n"}],"source":"tm.show_choices()"}
{"cell_type":"markdown","metadata":{"editable":false},"source":"#### STORE YOUR ANSWERS BY RUNNING THE FOLLOWING CELL"}
{"cell_type":"code","execution_count":62,"metadata":{"editable":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"\nYour choices have been saved. You can overwrite your results by running this cell again.\n"}],"source":"tm.save_choices()"}
{"cell_type":"markdown","metadata":{"editable":false},"source":"## Part 2 \n\nIn this section you will implement a CKY algorithm that produces all the possible trees of a sentence. Also, you can implement the probabilistic CKY for extra points. You must assume the following CNF grammar:\n\n```\n    S   -> NP VP [1.0]\n    NP  -> DT N  [0.5]  | NP PP     [0.25] | John [0.1] | I [0.15]\n    DT  -> the   [0.8]  | my        [0.2]\n    N   -> man   [0.5]  | telescope [0.5]\n    VP  -> VP PP [0.3]  | V NP      [0.7]\n    V   -> ate   [0.35] | saw       [0.65]\n    PP  -> P NP  [1.0]\n    P   -> with  [0.61] | under     [0.39]\n```\n\nYou **can use nltk utilities** for any intermmediate task, but the CKY algorithms must be your own code."}
{"cell_type":"markdown","metadata":{"editable":false},"source":"### 1 - (10pts)\n\nUsing the above grammar generate all the possible trees for the following sentence:\n\n    John saw a man with my telescope\n    \nYou can use your CKY code or implement any other approach to achieve the desired outcome."}
{"cell_type":"code","execution_count":3,"metadata":{"editable":false,"trusted":true},"outputs":[{"data":{"text/plain":"['John', 'saw', 'the', 'man', 'with', 'my', 'telescope']"},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":"sent = 'John saw the man with my telescope'.split()\nsent"}
{"cell_type":"code","execution_count":4,"metadata":{"editable":false,"trusted":true},"outputs":[],"source":"from nltk.grammar import PCFG\n\ngrammar = PCFG.fromstring(\"\"\"\n    S -> NP VP [1.0] \n    NP -> DT N [0.5] | NP PP [0.25] | 'John' [0.1] | 'I' [0.15]\n    DT -> 'the' [0.8] | 'my' [0.2]\n    N -> 'man' [0.5] | 'telescope' [0.5]\n    VP -> VP PP [0.3] | V NP [0.7]\n    V -> 'ate' [0.35] | 'saw' [0.65]\n    PP -> P NP [1.0] \n    P -> 'with' [0.61] | 'under' [0.39]\"\"\")"}
{"cell_type":"code","execution_count":16,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"8.3265e-05\n"}],"source":"### YOUR CODE GOES HERE\nfrom nltk import tree\nfrom nltk.tree import Tree\nfrom nltk.grammar import Nonterminal\ngrammar1 = open(\"grammar.txt\").readlines()\nword2 = open(\"grammar2.txt\").readlines()\n#print(grammar1)\ngrammar2 ={}\nfrom collections import defaultdict\nfor line in grammar1:\n    rule = line.strip('\\n').split('|')\n    rule0 = rule[0].strip().split('->')\n    nonparent1 = Nonterminal(rule0[0].strip())\n    rightside2 = rule0[1].strip().split()\n    tuple_right = (Nonterminal(rightside2[0]), Nonterminal(rightside2[1]))\n    rights = (tuple_right, float(rule[1]))\n    if nonparent1 in grammar2:\n        grammar2[nonparent1].append(rights)\n    else:\n        grammar2[nonparent1] = [rights]\ngrammar3 = defaultdict(dict)\n\nfor line in word2:\n    rule = line.strip('\\n').split('|')\n    rule0 = rule[0].strip().split('->')\n    nonparent2 = Nonterminal(rule0[0].strip())\n    rightside2 = rule0[1].strip()\n    for item in rightside2:\n        nonterminal2 = (rightside2,)\n        tuple2 = (nonterminal2, float(rule[1]))\n    if nonparent2 in grammar2:\n        grammar2[nonparent2].append(tuple2)\n    else:\n        grammar2[nonparent2] = [tuple2]\nfor left in grammar2:\n    for item in grammar2[left]:\n        grammar3[left][item[0]] = item[1]\ninversePCFG_non = defaultdict(list)\nfor key in grammar3:\n    for value in grammar3[key]:\n        inversePCFG_non[value].append((key,grammar3[key][value]))\n#print(inversePCFG_non)\n\ndef find_nts(word):\n    values = inversePCFG_non[(word,)]\n    result = [(Tree(item[0], [word, '']), item[1])for item in values]\n    return result\nlong = len(sent)\ntable = defaultdict(lambda: defaultdict(list))\nfor end in range(2, long + 2):\n    word9 = '\\''\n    word9 += sent[end-2]\n    word9 += '\\''\n    table[end - 2][end - 1] = find_nts(word9)\n    for start in range(end - 2, -1, -1):\n        for split in range(start + 1, end):\n            for left in table[start][split]:\n                for right in table[split][end - 1]:\n                    rules = inversePCFG_non[(left[0].label(), right[0].label())] \n                    table[start][end - 1].extend(\n                                [(Tree(item[0], [left[0], right[0]]), left[1] * right[1] * item[1]) for item in rules])\nif len(table[0][long]) == 0:\n    pass\nelse:\n    parse = sorted(table[0][long], key=lambda x: x[1])[-1]\n    prob = parse[1]\n    tree_max = parse[0]\n    parse[0]\ntrees = []\nfor item in table[0][long]:\n        trees.append(item[0])\nprint(parse[1])"}
{"cell_type":"code","execution_count":17,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Your trees have been saved. You can overwrite your results by running this function again.\n"}],"source":"tm.save_cky_trees(trees)"}
{"cell_type":"markdown","metadata":{"editable":false},"source":"### 2 - (10pts - BONUS!)\n\nProvide the most likely tree and its probability using your PCKY code."}
{"cell_type":"code","execution_count":18,"metadata":{"trusted":true},"outputs":[],"source":"parse_tree = ''\nparse_tree = parse[0]\nparse_prob = 0.0\nparse_prob += prob\n"}
{"cell_type":"markdown","metadata":{"editable":false},"source":"Save your results running the following function:"}
{"cell_type":"code","execution_count":19,"metadata":{"editable":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Your tree has been saved. You can overwrite your results by running this function again.\n"}],"source":"tm.save_pcky_tree(parse_tree, parse_prob)"}
{"cell_type":"markdown","metadata":{},"source":"---\n## Take a look at your answers"}
{"cell_type":"code","execution_count":20,"metadata":{"editable":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"1\t1\r\n2\t3\r\n3\t3\r\n4\t1\r\n5\t3\r\n6\t3\r\n7\t3\r\n8\t4\r\n"}],"source":"!cat multiple_choice.tsv"}
{"cell_type":"code","execution_count":20,"metadata":{"editable":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"(S (NP 'John' ) (VP (V 'saw' ) (NP (NP (DT 'the' ) (N 'man' )) (PP (P 'with' ) (NP (DT 'my' ) (N 'telescope' ))))))\r\n(S (NP 'John' ) (VP (VP (V 'saw' ) (NP (DT 'the' ) (N 'man' ))) (PP (P 'with' ) (NP (DT 'my' ) (N 'telescope' )))))\r\n"}],"source":"!cat cky_trees.txt"}
{"cell_type":"code","execution_count":21,"metadata":{"editable":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"(S (NP 'John' ) (VP (VP (V 'saw' ) (NP (DT 'the' ) (N 'man' ))) (PP (P 'with' ) (NP (DT 'my' ) (N 'telescope' )))))\t8.3265e-05\r\n"}],"source":"!cat pcky_tree.tsv"}
{"cell_type":"code","execution_count":0,"metadata":{"trusted":true},"outputs":[],"source":""}